{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPv4k4in2KGJ/GA8+DVlQse",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/taoducminh/AIO-Exercise/blob/main/Module_4_Week_1_Exercise_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7FUpJUAFgOe",
        "outputId": "359f9e36-fd28-4309-f74d-f00219dab64c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[624.1, 175.10000000000002, 300.5, 78.9]\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "# Function to get a specific column from the data\n",
        "def get_column(data, index):\n",
        "    result = [row[index] for row in data]  # Extract the column values based on the index\n",
        "    return result\n",
        "\n",
        "# Function to prepare the dataset for training\n",
        "def prepare_data(file_name_dataset):\n",
        "    data = np.genfromtxt(file_name_dataset, delimiter=',', skip_header=1).tolist()  # Load the data from the CSV\n",
        "    N = len(data)  # Get the number of rows\n",
        "\n",
        "    # Get TV, Radio, Newspaper, and Sales columns\n",
        "    tv_data = get_column(data, 0)       # TV data (index 0)\n",
        "    radio_data = get_column(data, 1)    # Radio data (index 1)\n",
        "    newspaper_data = get_column(data, 2)  # Newspaper data (index 2)\n",
        "    sales_data = get_column(data, 3)    # Sales data (index 3)\n",
        "\n",
        "    # Prepare the input (X) and output (y)\n",
        "    X = [tv_data, radio_data, newspaper_data]  # Input features\n",
        "    y = sales_data  # Output feature (sales)\n",
        "\n",
        "    return X, y\n",
        "\n",
        "# Simulating the question to match the correct multiple-choice answer\n",
        "X, y = prepare_data('/content/advertising.csv')  # Load the dataset\n",
        "\n",
        "# Sum the first 5 values for TV, Radio, Newspaper, and Sales\n",
        "list_result = [sum(X[0][:5]), sum(X[1][:5]), sum(X[2][:5]), sum(y[:5])]\n",
        "print(list_result)\n",
        "\n",
        "# Multiple-choice answers:\n",
        "# a) [624.1, 175.1, 300.5, 78.9]\n",
        "# b) [625, 175.1, 75.0, 7.2]\n",
        "# c) [626, 175.1, 75.0, 7.2]\n",
        "# d) [627.8, 175.1, 75.0, 7.2]\n",
        "\n",
        "# Compare the output to the choices to select the correct one.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def initialize_params():\n",
        "    w1 = random.gauss(mu=0.0, sigma=0.01)\n",
        "    w2 = random.gauss(mu=0.0, sigma=0.01)\n",
        "    w3 = random.gauss(mu=0.0, sigma=0.01)\n",
        "    b = 0\n",
        "    return w1, w2, w3, b\n",
        "\n",
        "def predict(x1, x2, x3, w1, w2, w3, b):\n",
        "    y_hat = w1 * x1 + w2 * x2 + w3 * x3 + b\n",
        "    return y_hat\n",
        "\n",
        "def compute_loss(y_hat, y):\n",
        "    loss = (y_hat - y) ** 2\n",
        "    return loss\n",
        "\n",
        "def compute_gradient_wi(xi, y, y_hat):\n",
        "    dl_dwi = 2 * xi * (y_hat - y)\n",
        "    return dl_dwi\n",
        "\n",
        "def compute_gradient_b(y, y_hat):\n",
        "    dl_db = 2 * (y_hat - y)\n",
        "    return dl_db\n",
        "\n",
        "def update_weight_wi(wi, dl_dwi, lr):\n",
        "    wi = wi - lr * dl_dwi\n",
        "    return wi\n",
        "\n",
        "def update_weight_b(b, dl_db, lr):\n",
        "    b = b - lr * dl_db\n",
        "    return b\n",
        "\n",
        "def implement_linear_regression(X_data, y_data, epoch_max=50, lr=1e-5):\n",
        "    losses = []\n",
        "\n",
        "    # Initialize parameters\n",
        "    w1, w2, w3, b = initialize_params()\n",
        "\n",
        "    N = len(y_data)  # Number of samples\n",
        "\n",
        "    # Gradient Descent Loop\n",
        "    for epoch in range(epoch_max):\n",
        "        for i in range(N):\n",
        "            # Get one sample\n",
        "            x1 = X_data[0][i]\n",
        "            x2 = X_data[1][i]\n",
        "            x3 = X_data[2][i]\n",
        "            y = y_data[i]\n",
        "\n",
        "            # Compute predicted output\n",
        "            y_hat = predict(x1, x2, x3, w1, w2, w3, b)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = compute_loss(y_hat, y)\n",
        "\n",
        "            # Compute gradients\n",
        "            dl_dw1 = compute_gradient_wi(x1, y, y_hat)\n",
        "            dl_dw2 = compute_gradient_wi(x2, y, y_hat)\n",
        "            dl_dw3 = compute_gradient_wi(x3, y, y_hat)\n",
        "            dl_db = compute_gradient_b(y, y_hat)\n",
        "\n",
        "            # Update parameters\n",
        "            w1 = update_weight_wi(w1, dl_dw1, lr)\n",
        "            w2 = update_weight_wi(w2, dl_dw2, lr)\n",
        "            w3 = update_weight_wi(w3, dl_dw3, lr)\n",
        "            b = update_weight_b(b, dl_db, lr)\n",
        "\n",
        "        # Logging loss for each epoch\n",
        "        losses.append(loss)\n",
        "\n",
        "    return w1, w2, w3, b, losses"
      ],
      "metadata": {
        "id": "nBD4fnwYJHbV"
      },
      "execution_count": 2,
      "outputs": []
    }
  ]
}